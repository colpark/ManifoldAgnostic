{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Neural Field Diffusion: Training and Testing\n\nThis notebook trains and tests the neural field diffusion model on toy point cloud data.\n\n**Key Innovation**: Unlike traditional flow matching that predicts velocities at discrete sample points,\nwe learn a **continuous vector field** `v_θ: ℝ³ × [0,T] → ℝ³` that can be queried at ANY spatial location.\n\n**Architecture (PixNerd-style)**:\n1. **Global DiT Blocks**: Points → Shape context `s` (with 3D RoPE, AdaLN, SwiGLU)\n2. **NerfBlocks (HyperNetwork)**: `s` → MLP weights (with weight normalization)\n3. **Neural Field**: `(x, t, weights)` → Velocity `v(x, t)`\n\n**Key Components from PixNerd**:\n- RMSNorm for efficient normalization\n- SwiGLU feedforward networks\n- 3D Rotary Position Embeddings (adapted from 2D)\n- AdaLN modulation for condition injection\n- HyperNetwork with weight normalization for stable training\n\n**Important**: Each point is treated as a token, so this is O(N²) attention. We use 256-512 points for testing."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nsys.path.append('..')\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom tqdm.notebook import tqdm\nimport time\n\n# Our modules\nfrom data.toy_data import (\n    get_all_generators, generate_sphere, generate_torus,\n    generate_helix, generate_multi_sphere_ring, generate_multi_sphere_cube,\n    PointCloud, ManifoldDim\n)\nfrom src.models.neural_field import NeuralFieldDiffusion\nfrom src.diffusion.flow_matching import FlowMatchingLoss, FlowMatchingSampler\n\n%matplotlib inline\nplt.style.use('seaborn-v0_8-whitegrid')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Configuration\n\n**Key Settings**:\n- `N_POINTS = 256-512` (each point is a token)\n- `SHAPES = ['torus']` - use shapes with variation, or multi-shape training\n- `RANDOM_TRANSFORM = True` - creates variation via rotation + anisotropic scaling\n- **SMALL MODEL** - ~300K params is enough for toy data (vs. 10M+ for real data)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# CONFIGURATION - Adjust these as needed\n# =============================================================================\n\n# Data - multi_sphere_ring has structured variation (8 spheres with jitter)\nSHAPES = ['multi_sphere_ring']  # Try: ['multi_sphere_cube'], ['torus'], or multi-shape\nN_POINTS = 512           # Points per cloud (512 for 8 spheres = 64 per sphere)\nN_SAMPLES = 1000         # Training samples\nRANDOM_TRANSFORM = False # multi_sphere already has built-in variation!\nSCALE_RANGE = (0.7, 1.3) # Only used if RANDOM_TRANSFORM=True\n\n# Model (SMALL for toy data - PixNerd-style architecture)\nHIDDEN_SIZE = 128        # Transformer hidden dimension (small!)\nHIDDEN_SIZE_X = 32       # NerfBlock hidden dimension\nNUM_HEADS = 4            # Number of attention heads\nNUM_BLOCKS = 6           # Total blocks (2 DiT + 4 NerfBlocks)\nNUM_COND_BLOCKS = 2      # DiT blocks (rest are NerfBlocks)\nNERF_MLP_RATIO = 2       # MLP ratio for NerfBlocks\nMAX_FREQS = 6            # Fourier frequency bands\n\n# Training\nEPOCHS = 300             # Training epochs\nBATCH_SIZE = 32          # Batch size\nLR = 1e-4                # Learning rate\n\n# Device\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {DEVICE}\")\nprint(f\"Training on: {SHAPES}\")\nprint(f\"Random transform: {RANDOM_TRANSFORM} (multi_sphere has built-in variation)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class PointCloudDataset(Dataset):\n    \"\"\"Dataset of point clouds with random transforms for variation.\"\"\"\n    \n    def __init__(self, shapes, n_samples, n_points, \n                 noise_std=0.001, random_transform=True, scale_range=(0.7, 1.3)):\n        self.shapes = shapes if isinstance(shapes, list) else [shapes]\n        self.n_samples = n_samples\n        self.n_points = n_points\n        self.noise_std = noise_std\n        self.random_transform = random_transform\n        self.scale_range = scale_range\n        \n        all_generators = get_all_generators()\n        self.generators = [all_generators[s] for s in self.shapes]\n    \n    def __len__(self):\n        return self.n_samples\n    \n    def __getitem__(self, idx):\n        generator = self.generators[idx % len(self.generators)]\n        pc = generator(n_points=self.n_points)\n        \n        # Apply random transforms for variation!\n        if self.random_transform:\n            pc = pc.random_transform(\n                rotate=True,\n                scale_range=self.scale_range,\n                anisotropic=True  # sphere -> ellipsoid, etc.\n            )\n        \n        pc = pc.normalize()\n        if self.noise_std > 0:\n            pc = pc.add_noise(self.noise_std)\n        return torch.tensor(pc.points, dtype=torch.float32)\n\n# Create dataset and dataloader\ndataset = PointCloudDataset(\n    SHAPES, N_SAMPLES, N_POINTS,\n    random_transform=RANDOM_TRANSFORM,\n    scale_range=SCALE_RANGE\n)\ndataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n\nprint(f\"Dataset: {SHAPES}\")\nprint(f\"Samples: {N_SAMPLES}\")\nprint(f\"Points per sample: {N_POINTS}\")\nprint(f\"Random transform: {RANDOM_TRANSFORM}\")\nprint(f\"Batches per epoch: {len(dataloader)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize training samples - note the VARIATION in sphere positions!\nfig = plt.figure(figsize=(16, 4))\nfor i in range(4):\n    sample = dataset[i].numpy()\n    ax = fig.add_subplot(1, 4, i+1, projection='3d')\n    # Color by z to show structure\n    colors = sample[:, 2]\n    ax.scatter(sample[:, 0], sample[:, 1], sample[:, 2], \n               c=colors, cmap='viridis', s=3, alpha=0.7)\n    ax.set_title(f'Sample {i+1}')\n    ax.set_xlim([-1.2, 1.2])\n    ax.set_ylim([-1.2, 1.2])\n    ax.set_zlim([-1.2, 1.2])\n    ax.view_init(elev=20, azim=30 + i*20)\n\nshapes_str = ', '.join(SHAPES)\nplt.suptitle(f'Training Data: {shapes_str} ({N_POINTS} pts) - Each sample has DIFFERENT sphere positions!', fontsize=12)\nplt.tight_layout()\nplt.show()\n\nprint(\"Multi-sphere variation comes from:\")\nprint(\"  - Position jitter: ±15% offset from base positions\")\nprint(\"  - Radius jitter: ±30% variation in sphere sizes\")\nprint(\"  - Ring rotation: random angle offset each sample\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create SMALL model (PixNerd-style: DiT blocks + NerfBlocks)\nmodel = NeuralFieldDiffusion(\n    in_channels=3,\n    out_channels=3,\n    hidden_size=HIDDEN_SIZE,\n    hidden_size_x=HIDDEN_SIZE_X,\n    num_heads=NUM_HEADS,\n    num_blocks=NUM_BLOCKS,\n    num_cond_blocks=NUM_COND_BLOCKS,\n    nerf_mlp_ratio=NERF_MLP_RATIO,\n    max_freqs=MAX_FREQS,\n).to(DEVICE)\n\n# Count parameters\nn_params = sum(p.numel() for p in model.parameters())\nprint(f\"Model: NeuralFieldDiffusion (PixNerd-style, SMALL)\")\nprint(f\"Architecture: {NUM_COND_BLOCKS} DiT blocks + {NUM_BLOCKS - NUM_COND_BLOCKS} NerfBlocks\")\nprint(f\"Parameters: {n_params:,}\")\nprint(f\"  (For reference: ~300K is fine for toy data, ~10M+ for real data)\")\n\n# Test forward pass\ntest_input = torch.randn(2, N_POINTS, 3, device=DEVICE)\ntest_t = torch.rand(2, device=DEVICE)\ntest_output = model(test_input, test_t)\nprint(f\"\\nInput shape: {test_input.shape}\")\nprint(f\"Output shape: {test_output.shape}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01)\n",
    "loss_fn = FlowMatchingLoss(schedule_type='linear')\n",
    "sampler = FlowMatchingSampler(model)\n",
    "\n",
    "# Training history\n",
    "train_losses = []\n",
    "epoch_times = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, loss_fn, device):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    n_batches = 0\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        x0 = batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = loss_fn(model, x0)\n",
    "        loss = output['loss']\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        n_batches += 1\n",
    "    \n",
    "    return total_loss / n_batches\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_samples(model, sampler, n_samples=4, n_points=256, n_steps=50, device='cpu'):\n",
    "    \"\"\"Generate samples from the model.\"\"\"\n",
    "    model.eval()\n",
    "    noise = torch.randn(n_samples, n_points, 3, device=device)\n",
    "    samples = sampler.sample_euler(noise, n_steps=n_steps)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "print(f\"Training for {EPOCHS} epochs...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "pbar = tqdm(range(EPOCHS), desc=\"Training\")\n",
    "\n",
    "for epoch in pbar:\n",
    "    start_time = time.time()\n",
    "    loss = train_epoch(model, dataloader, optimizer, loss_fn, DEVICE)\n",
    "    epoch_time = time.time() - start_time\n",
    "    \n",
    "    train_losses.append(loss)\n",
    "    epoch_times.append(epoch_time)\n",
    "    \n",
    "    pbar.set_postfix({'loss': f'{loss:.4f}'})\n",
    "    \n",
    "    # Log every 50 epochs\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        tqdm.write(f\"Epoch {epoch+1}/{EPOCHS} | Loss: {loss:.4f} | Time: {epoch_time:.2f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"Training complete!\")\n",
    "print(f\"Final loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"Average epoch time: {np.mean(epoch_times):.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curve\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_losses[10:])  # Skip first few for better scale\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss (after warmup)')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples\n",
    "print(\"Generating samples...\")\n",
    "samples = generate_samples(model, sampler, n_samples=8, n_points=N_POINTS, \n",
    "                           n_steps=50, device=DEVICE)\n",
    "samples = samples.cpu().numpy()\n",
    "print(f\"Generated {samples.shape[0]} samples with {samples.shape[1]} points each\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compare ground truth vs generated\nfig = plt.figure(figsize=(16, 8))\n\n# Ground truth (top row) - showing variation\nfor i in range(4):\n    gt = dataset[i].numpy()\n    ax = fig.add_subplot(2, 4, i + 1, projection='3d')\n    ax.scatter(gt[:, 0], gt[:, 1], gt[:, 2], s=2, alpha=0.5, c='blue')\n    ax.set_title(f'Ground Truth {i+1}')\n    ax.set_xlim([-1.5, 1.5])\n    ax.set_ylim([-1.5, 1.5])\n    ax.set_zlim([-1.5, 1.5])\n\n# Generated (bottom row)\nfor i in range(4):\n    ax = fig.add_subplot(2, 4, i + 5, projection='3d')\n    ax.scatter(samples[i, :, 0], samples[i, :, 1], samples[i, :, 2], \n               s=2, alpha=0.5, c='red')\n    ax.set_title(f'Generated {i+1}')\n    ax.set_xlim([-1.5, 1.5])\n    ax.set_ylim([-1.5, 1.5])\n    ax.set_zlim([-1.5, 1.5])\n\nshapes_str = ', '.join(SHAPES)\nplt.suptitle(f'{shapes_str}: GT (blue, varied) vs Generated (red)', fontsize=14)\nplt.tight_layout()\nplt.show()\n\nprint(\"Note: GT samples show variation from random transforms.\")\nprint(\"Model should learn to generate similar variety.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Resolution Independence Test\n\n**Key Point**: This is UNCONDITIONAL generation - no reference shape needed!\n\nThe model should generate valid shapes starting from noise at ANY resolution.\nWe just call `sample_euler` with different sized noise tensors."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# Resolution Independence Test - UNCONDITIONAL GENERATION\n# =============================================================================\n# No reference shape! Just generate from noise at different resolutions.\n# The model's forward() handles any number of points.\n\nprint(\"Testing UNCONDITIONAL resolution independence...\")\nprint(\"(No reference shape - pure generation from noise)\\n\")\n\nresolutions = [64, 128, 256, 512, 1024]\n\nfig = plt.figure(figsize=(20, 4))\n\nfor i, n_pts in enumerate(resolutions):\n    # Start from noise at this resolution\n    noise = torch.randn(1, n_pts, 3, device=DEVICE)\n    \n    # Generate using standard Euler sampling - NO reference needed!\n    with torch.no_grad():\n        model.eval()\n        sample = sampler.sample_euler(noise, n_steps=50)\n    \n    sample = sample.cpu().numpy()[0]\n    \n    ax = fig.add_subplot(1, 5, i + 1, projection='3d')\n    ax.scatter(sample[:, 0], sample[:, 1], sample[:, 2], \n               s=max(1, 5 - i), alpha=0.5, c='green')\n    ax.set_title(f'N = {n_pts}')\n    ax.set_xlim([-1.5, 1.5])\n    ax.set_ylim([-1.5, 1.5])\n    ax.set_zlim([-1.5, 1.5])\n    \n    print(f\"  Generated {n_pts:5d} points (unconditional, from noise)\")\n\nplt.suptitle('Unconditional Resolution Independence: Same Model, Any Number of Points', fontsize=14)\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"This is TRUE resolution independence:\")\nprint(\"- No reference shape\")\nprint(\"- Pure unconditional generation\")\nprint(\"- Model handles any N because attention works on variable length\")\nprint(\"=\"*60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generation Process Visualization\n",
    "\n",
    "Watch the ODE integration transform noise into the target manifold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_with_trajectory(model, sampler, n_points=256, n_steps=50, device='cpu'):\n",
    "    \"\"\"Generate with full trajectory.\"\"\"\n",
    "    model.eval()\n",
    "    noise = torch.randn(1, n_points, 3, device=device)\n",
    "    trajectory = sampler.sample_euler(noise, n_steps=n_steps, return_trajectory=True)\n",
    "    return trajectory\n",
    "\n",
    "# Generate trajectory\n",
    "trajectory = generate_with_trajectory(model, sampler, n_points=N_POINTS, \n",
    "                                      n_steps=50, device=DEVICE)\n",
    "trajectory = trajectory.cpu().numpy()[:, 0]  # [steps, N, 3]\n",
    "\n",
    "# Visualize at selected timesteps\n",
    "n_steps = trajectory.shape[0]\n",
    "step_indices = [0, n_steps//4, n_steps//2, 3*n_steps//4, n_steps-1]\n",
    "t_values = [1.0, 0.75, 0.5, 0.25, 0.0]\n",
    "\n",
    "fig = plt.figure(figsize=(20, 4))\n",
    "\n",
    "for i, (step_idx, t_val) in enumerate(zip(step_indices, t_values)):\n",
    "    points = trajectory[step_idx]\n",
    "    \n",
    "    ax = fig.add_subplot(1, 5, i + 1, projection='3d')\n",
    "    color = plt.cm.coolwarm(1 - t_val)\n",
    "    ax.scatter(points[:, 0], points[:, 1], points[:, 2], s=2, alpha=0.5, c=[color])\n",
    "    ax.set_title(f't = {t_val:.2f}')\n",
    "    ax.set_xlim([-1.5, 1.5])\n",
    "    ax.set_ylim([-1.5, 1.5])\n",
    "    ax.set_zlim([-1.5, 1.5])\n",
    "\n",
    "plt.suptitle('Generation Process: Noise (t=1) → Manifold (t=0)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test on Different Shapes\n",
    "\n",
    "Let's test the model (without retraining) by using context from different shapes.\n",
    "\n",
    "This shows how the shape context drives generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get some different shapes for context transfer test\ntest_shapes = ['sphere', 'torus', 'helix', 'trefoil_knot']\ngenerators = get_all_generators()\n\nfig = plt.figure(figsize=(16, 8))\n\nfor i, shape_name in enumerate(test_shapes):\n    # Generate a reference point cloud\n    pc = generators[shape_name](n_points=N_POINTS).normalize()\n    ref_points = torch.tensor(pc.points, dtype=torch.float32, device=DEVICE).unsqueeze(0)\n    \n    # Get context from this shape\n    ref_t = torch.zeros(1, device=DEVICE)\n    context = model.get_context(ref_points, ref_t)\n    \n    # Generate using this context\n    with torch.no_grad():\n        model.eval()\n        generated = sampler.sample_at_resolution(context, n_points=N_POINTS, n_steps=50)\n    generated = generated.cpu().numpy()[0]\n    ref_np = ref_points.cpu().numpy()[0]\n    \n    # Plot reference\n    ax = fig.add_subplot(2, 4, i + 1, projection='3d')\n    ax.scatter(ref_np[:, 0], ref_np[:, 1], ref_np[:, 2], s=2, alpha=0.5, c='blue')\n    ax.set_title(f'{shape_name} (input)')\n    ax.set_xlim([-1.5, 1.5])\n    ax.set_ylim([-1.5, 1.5])\n    ax.set_zlim([-1.5, 1.5])\n    \n    # Plot generated\n    ax = fig.add_subplot(2, 4, i + 5, projection='3d')\n    ax.scatter(generated[:, 0], generated[:, 1], generated[:, 2], s=2, alpha=0.5, c='red')\n    ax.set_title(f'{shape_name} (generated)')\n    ax.set_xlim([-1.5, 1.5])\n    ax.set_ylim([-1.5, 1.5])\n    ax.set_zlim([-1.5, 1.5])\n\nshapes_str = ', '.join(SHAPES)\nplt.suptitle(f'Context Transfer Test (model trained on {shapes_str})', fontsize=14)\nplt.tight_layout()\nplt.show()\n\nprint(f\"Note: Model was trained ONLY on '{shapes_str}'.\")\nprint(\"Generation quality depends on how well the learned field generalizes.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 9. Summary\n\n### What We Demonstrated:\n\n1. **Multi-Object Scenes**: 8 spheres with position/radius jitter creates meaningful variation\n\n2. **Spatial Relationship Learning**: Model learns:\n   - Number and arrangement of spheres\n   - Relative positions (ring pattern)\n   - Variation in positions and sizes\n\n3. **Small Model**: ~300K params is sufficient for toy data (vs 10M+ for complex real-world shapes)\n\n4. **Neural Field Architecture** (PixNerd-style): \n   - DiT blocks with 3D RoPE, AdaLN, SwiGLU for global context\n   - NerfBlocks (hyper-network with weight normalization) for local field\n\n5. **Resolution Independence**: Same model generates at 64, 256, or 1024 points\n\n### Key Observations:\n\n- Multi-sphere has built-in variation (no random transforms needed)\n- Model must learn spatial relationships, not just single shape geometry\n- Position jitter (±15%) and radius jitter (±30%) create unique samples\n\n### Next Steps:\n\n1. Try other arrangements: `SHAPES = ['multi_sphere_cube']` or `['multi_sphere_random']`\n2. Try single shapes with transforms: `SHAPES = ['torus']` with `RANDOM_TRANSFORM = True`\n3. Add conditional generation (shape class labels)\n4. Scale to more complex shapes (ShapeNet)\n5. Extract geometric information (normals, SDF) from learned field"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Save model checkpoint\nshapes_str = '_'.join(SHAPES)\ncheckpoint = {\n    'model_state_dict': model.state_dict(),\n    'config': {\n        'hidden_size': HIDDEN_SIZE,\n        'hidden_size_x': HIDDEN_SIZE_X,\n        'num_heads': NUM_HEADS,\n        'num_blocks': NUM_BLOCKS,\n        'num_cond_blocks': NUM_COND_BLOCKS,\n        'nerf_mlp_ratio': NERF_MLP_RATIO,\n        'max_freqs': MAX_FREQS,\n    },\n    'train_losses': train_losses,\n    'shapes': SHAPES,\n    'n_points': N_POINTS,\n    'random_transform': RANDOM_TRANSFORM,\n}\n\ntorch.save(checkpoint, f'../experiments/outputs/notebook_checkpoint_{shapes_str}.pt')\nprint(f\"Saved checkpoint to ../experiments/outputs/notebook_checkpoint_{shapes_str}.pt\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}